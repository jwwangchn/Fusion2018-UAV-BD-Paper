\section{Baselines and Methods}
\label{sec:exp}

%% 1. 简要再次介绍数据集

The data for all experiments comes from UAV-BD. And the numbers of images for the training, validation and testing date are $ 16258 $, $ 4065 $ and $ 5081 $ images, respectively. The images for training, testing and validation are the size of $ 342 \times 342 $. The whole UAV-BD contains $ 16258 $ images with $ 22211 $ instances for training, $ 5081 $ images with $ 6944 $ instances for testing and $ 4055 $ images with $ 5624 $ instances for validation.

%% 2. 实验使用了 4 中基于深度学习的方法

Here, we compare three kinds of approaches which differ in the use of detection framework and data annotating method. For horizontal object detection, we select Faster R-CNN\footnote{\url{https://github.com/rbgirshick/py-faster-rcnn.git}}\cite{FasterRCNN},  SSD\footnote{\url{https://github.com/weiliu89/caffe.git}}\cite{SSD} and YOLOv2\footnote{\url{https://github.com/weiliu89/caffe.git}}\cite{SSD} as our baseline testing algorithms for their excellent performance on general object detection. For oriented object detection, we improve the original Rotation Region Proposal Networks(RRPN) algorithm\cite{RRPN} to predict properly oriented bounding boxes. This method can be denoted as $ \{c_x, c_y, w, h, \theta\} $, where ($ c_x $, $ c_y $) is the central coordinate of the oriented bounding box, $ w $, $ h $ and $ \theta $ is the width, height and rotation angle of the oriented bounding box respectively.

%\begin{itemize}
%	
%	\item \textbf{Faster R-CNN}
%	
%	\item \textbf{SSD}
%	
%	\item \textbf{RRPN}
%	
%	\item \textbf{DRBox}
%\end{itemize}


\subsection{Baselines with Horizontal Bounding Boxes}

Ground truths for horizontal bounding boxes(HBB) experiments are generated by calculating the axis-aligned bounding boxes over original bounding boxes. To make it fair, we keep all the experiments' setting and hyper-parameters the same as depicted in corresponding papers\cite{FasterRCNN, SSD, YOLOv2}.

The experimental results of HBB prediction are shown in Fig.\ref{fig:pr_bbox}. In Fig.\ref{fig:pr_bbox}, the blue one illustrates the results for Faster R-CNN, the orange one illustrates the results for SSD and the green one illustrates the results for YOLOv2.


\begin{figure}
	\includegraphics[width=\linewidth]{images/pr_bbox.pdf}
	\caption{Numerical results (AP) of baseline models evaluated with HBB ground truths.}
	\label{fig:pr_bbox}
\end{figure}

\subsection{Baseline with Oriented Bounding Boxes}

Prediction of oriented bounding boxes(OBB) is difficult, because the present state-of-the-art detection methods are not designed for oriented objects. Therefore, we choose Rotation Region Proposal Networks(RRPN)\cite{RRPN} as the framework for its accuracy and efficiency. Then we modify it to adapt UAV-BD on the basis of dataset statistics in section \ref{ssec:Dataset_Statistics}.

RRPN is based on Faster R-CNN. For Faster R-CNN, the Region of Interests (RoIs) is generated by Region Proposal Network(RPN), and the RoIs is rectangle which can be written as $ R = (x_{min}, y_{min}, x_{max}, y_{max}) = (c_x, c_y, w, h) $. These RoIs have regressed from $ k $ anchors which are generated by some predefined scales and aspect ratios. But in RRPN, except for predefined scales and aspect ratios, it also uses \textit{angles} to generate RoIs. That is the reason why RRPN can predict oriented bounding boxes which can be written as $ R=(c_x, c_y, w, h, \theta) $. In the section \ref{ssec:Dataset_Statistics}, we analyze the size, aspect ratio and angle distributions of UAV-BD, so we can select reasonable scales, aspect ratios and angles value to generate new anchors which are shown in Fig.\ref{fig:anchors}. The experimental results of RRPN, SSD, Faster R-CNN and YOLOv2 based on OBB are shown in Fig.\ref{fig:pr_rbbox}. The red one shows the results for RRPN.



\begin{figure}
	\includegraphics[width=\linewidth]{images/scale_ratio_angle.pdf}
	\caption{Anchor strategy in our framework of RRPN}
	\label{fig:anchors}
\end{figure}

\begin{figure}
	\includegraphics[width=\linewidth]{images/pr_rbbox.pdf}
	\caption{Numerical results (AP) of baseline models evaluated with OBB ground truths.}
	\label{fig:pr_rbbox}
\end{figure}

%\begin{figure}
%	\includegraphics[width=\linewidth]{images/RRPN.pdf}
%	\caption{Rotation based text detection pipline}
%	\label{fig:pipline}
%\end{figure}


\subsection{Experimental Analysis}

We use precision-recall curve and AP values to compare  three kinds of baseline models (Faster R-CNN, SSD and RRPN), which are evaluated with HBB ground truth. For evaluation metrics, we use the same AP calculation as for PASCAL VOC. As Fig.\ref{fig:pr_bbox} shown, the AP values of Faster R-CNN, SSD, YOLOv2 are $ 90.3\% $, $ 90.1\% $, $ 77.4\% $, respectively.

Then we still use the same evaluation indicators to compare four kinds of baseline models (Faster R-CNN, SSD, RRPN and YOLOv2). The difference is that these models are evaluated with OBB ground truth. As Fig.\ref{fig:pr_rbbox} shown, the AP values of RRPN, SSD, Faster R-CNN and YOLOv2 are $ 88.6\% $, $ 87.6\% $, $ 86.4\% $, $ 67.3\% $, respectively. It should be noticed that when using OBB for annotation, the performance of the three baseline methods decrease compared with that using HBB. We can clearly observe that the results of RRPN are the best.

In Fig.\ref{fig:result}, we compare the results between objects detection experiments based on HBB and OBB. For oriented objects shown in Fig.\ref{fig:result}, location precision of objects in HBB are much lower than that of OBB, and results are suppressed through progress operations. So we can get the conclusion that OBB regression is the correct way for oriented object detection. And OBB regression makes it possible for oriented object detection to be really integrated to applications.In addition, the results of RRPN show the highest location accuracy and the the lowest false-alarm and false positive.


\begin{figure}
	\includegraphics[width=\linewidth]{images/result.png}
	\caption{Visualization results of testing on UAV-BD using well-trained Faster R-CNN, SSD and RRPN. \textbf{TOP} to \textbf{Bottom} respectively illustrate the results for Faster R-CNN, SSD, YOLOv2 and RRPN.}
	\label{fig:result}
\end{figure}










%\begin{figure}
%	\centering
%	\subfigure[angle distribution]
%	{
%		\label{angle}
%		\includegraphics[width=0.45\linewidth]{images/pr_bbox.pdf}
%	}
%	\subfigure[size distribution]
%	{
%		\label{size}
%		\includegraphics[width=0.45\linewidth]{images/pr_rbbox.pdf}
%	}
%	\caption{The precision-recall curve for bottle detection for Faster R-CNN, SSD and RRPN}
%	\label{fig:PR}
%\end{figure}





%\renewcommand{\algorithmicrequire}{\textbf{Input:}}
%\renewcommand{\algorithmicensure}{\textbf{Output:}}


%\begin{algorithm}
%	\caption{IoU computation}
%	\begin{algorithmic}[1]
%		\Require Rectangle $ R_1, R_2,\cdots,R_N $
%		\State IoU[1, $ N $][1, $ N $] $ \gets $ 0
%		\For {each pair of $ R_i, R_j(i<j) $} 
%		\State Point set $ PSet \gets \emptyset $
%		\State Add intersection points of $ R_i $ and $ R_j $ to $ PSet $
%		\State Add the vertices of $ R_i $ inside $ R_j $ into $ PSet $
%		\State $ \text{IoU}(i,j) \gets (\text{Area}(R_i) + \text{Area}(R_j) - I)/I $
%		\EndFor
%		\Ensure IoU
%	\end{algorithmic}
%\end{algorithm}

